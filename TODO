DONE Change the gatting from sigmoid to tanh to have more like control net, no impact from side in the beggining
DONE Add final skip connection
DONE Correct the last hidden to side ladder which where wrong by one
DONE implement small transformer head on top instead of MLP
DONE test small transformer head to see if possible (WARNING already quite big)
DONE Change the 0 shot classification task to higher level class to test
DONE train LST_text on MSCOCO 
retrain LST_text on MSCOCO with same epoch as LiT for fair comparison
IN PROCESS train LiT on MSCOCO to compare
DONE implement LoRA clip model 
# Not needed train LoRA model

1 Transformer Head with 1 layer on flickr (if good, we continue with LST with smaller layer, for example only 2 layers 1&5 vs 4&8)
2 train LST 8 layer on mscoco
Check if we go with smaller number of layer
Bert base uncased

1 use bigger data set cc3m
2 use bigger text encoder for LST for the same memory budget than the current LiT text encoder 
If still not beating, LoRA instead of LST
at the end use bigger image ViT for both (LiT and LST)
