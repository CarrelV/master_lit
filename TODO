DONE Change the gatting from sigmoid to tanh to have more like control net, no impact from side in the beggining
DONE Add final skip connection
DONE Correct the last hidden to side ladder which where wrong by one
DONE implement small transformer head on top instead of MLP
DONE test small transformer head to see if possible (WARNING already quite big)
DONE Change the 0 shot classification task to higher level class to test
DONE train LST_text on MSCOCO 
DONE retrain LST_text on MSCOCO with same epoch as LiT for fair comparison
DONE train LiT on MSCOCO to compare
DONE implement LoRA clip model 

DONE correct transformer head error in config

DONE train Transformer Head with 1 layer on flickr (if good, we continue with LST with smaller layer, for example only 2 layers 1&5 vs 4&8)

DONE Check if we go with smaller number of layer
DONE Bert base uncased

IN PROCESS use bigger data set cc3m
DONE use bigger text encoder for LST for the same memory budget than the current LiT text encoder 

DONE train base BERT on flickr30k
DONE implement LST for ViT
DONE train complete LST small on flickr
DONE train cutted LST small on flickr
DONE LST on MSCOCO with all ladder
IN PROCESS train LST on MSCOCO with cut ladder 
IN PROCESS train image-LST on flickr with all ladder
IN PROCESS train image-lST on flickr with cut ladder
train image-LST on MSCOCO with all ladder
train image-LST on MSCOCO with cut ladder

train LiT baseBART on MSCOCO (probably not possible)
DONE train APE on MS COCO

train LORA on Flickr/MSCOCO?

Search SOTA/literature for task which require image and text together
LST fusion?


Working closely with LilT as reference:
https://arxiv.org/pdf/2303.11866.pdf

DONE Their second possibility is just adding a head on top of the last ladder -> equivalent to APE, but with Transformer instead of MLP -> equivalent to our "transformer_head" architecture. So we're doing it already 

First possibility is LayerWise adapters -> to implement: before each layerNorm: add a downsampler, Gelu activation, upsampler and a residual connection accros it

Unlock layerNorm param as well

Implement I2T and T2I retrieval on COCO14

TODO:
- train without pruned weights to see importance of initialized side
Initialize downsampler with PCA
Initialize downsampler with identity from pruning
Upsampler: get a linear mapping from output B11' to B11

- Dense task: CLIP is good for dense task like semantic segmentation. If we can show that we have this nice thing but LiT and LilT not, that would be good. Linear probing

I,T,+ LST training on MSCOCO
On ADE20k and mscoco instance segmentation:
I(frozen) + LST (frozen) + Decoder
vs
I(frozen) (LiT so ViT is frozen anyway) + Decoder
Hopefully show that we're better

- Multimodal:
