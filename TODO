DONE Change the gatting from sigmoid to tanh to have more like control net, no impact from side in the beggining
DONE Add final skip connection
DONE Correct the last hidden to side ladder which where wrong by one
DONE implement small transformer head on top instead of MLP
DONE test small transformer head to see if possible (WARNING already quite big)
DONE Change the 0 shot classification task to higher level class to test
DONE train LST_text on MSCOCO 
DONE retrain LST_text on MSCOCO with same epoch as LiT for fair comparison
DONE train LiT on MSCOCO to compare
DONE implement LoRA clip model 

DONE correct transformer head error in config

DONE train Transformer Head with 1 layer on flickr (if good, we continue with LST with smaller layer, for example only 2 layers 1&5 vs 4&8)

DONE Check if we go with smaller number of layer
DONE Bert base uncased

IN PROCESS use bigger data set cc3m
DONE use bigger text encoder for LST for the same memory budget than the current LiT text encoder 

DONE train base BERT on flickr30k
DONE implement LST for ViT
DONE train complete LST small on flickr
DONE train cutted LST small on flickr
DONE LST on MSCOCO with all ladder
IN PROCESS train LST on MSCOCO with cut ladder 
IN PROCESS train image-LST on flickr with all ladder
IN PROCESS train image-lST on flickr with cut ladder
train image-LST on MSCOCO with all ladder
train image-LST on MSCOCO with cut ladder

train LiT baseBART on MSCOCO  
DONE train APE on MS COCO

train LORA on Flickr/MSCOCO?

Search SOTA/literature for task which require image and text together
LST fusion?