{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import config as CFG\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from transformers import BertModel, BertConfig\n",
    "from transformers import ViTModel, ViTConfig\n",
    "import wandb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "### No modification same as in models.py\n",
    "\n",
    "###################### TEXT TOWER ####################################\n",
    "\n",
    "class TextEncoder(nn.Module):\n",
    "    def __init__(self, model_name=CFG.text_model_name, pretrained=CFG.pretrained, trainable=CFG.trainable):\n",
    "        super().__init__()\n",
    "        if pretrained:\n",
    "            self.model = BertModel.from_pretrained(model_name)\n",
    "        else:\n",
    "\n",
    "            self.model = BertModel(config=BertConfig.from_pretrained(model_name))\n",
    "            \n",
    "        for p in self.model.parameters():\n",
    "            p.requires_grad = trainable\n",
    "\n",
    "        # we are using the CLS token hidden representation as the sentence's embedding\n",
    "        self.target_token_idx = 0\n",
    "\n",
    "    def forward(self, input_ids, attention_mask):\n",
    "        output = self.model(input_ids=input_ids, attention_mask=attention_mask)\n",
    "        last_hidden_state = output.last_hidden_state\n",
    "        return last_hidden_state[:, self.target_token_idx, :]\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "###################### IMAGE TOWER ####################################\n",
    "\n",
    "\n",
    "class ImageEncoder(nn.Module):\n",
    "    def __init__(self, model_name=CFG.image_model_name, pretrained=CFG.pretrained, trainable=CFG.trainable):\n",
    "        super().__init__()\n",
    "        if pretrained:\n",
    "            self.model = ViTModel.from_pretrained(model_name)\n",
    "        else:\n",
    "            self.model = ViTModel(config=ViTConfig.from_pretrained(model_name))\n",
    "            \n",
    "        for p in self.model.parameters():\n",
    "            p.requires_grad = trainable\n",
    "\n",
    "        self.target_token_idx = 0\n",
    "\n",
    "    def forward(self, image):\n",
    "        \n",
    "        output = self.model(image)\n",
    "        last_hidden_state = output.last_hidden_state\n",
    "        return last_hidden_state[:, self.target_token_idx, :]\n",
    "\n",
    "###################### PROJECTION HEAD on top ####################################\n",
    "\n",
    "class ProjectionHead(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        embedding_dim,\n",
    "        projection_dim=CFG.projection_dim,\n",
    "        dropout=CFG.dropout\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.projection = nn.Linear(embedding_dim, projection_dim)\n",
    "        self.gelu = nn.GELU()\n",
    "        self.fc = nn.Linear(projection_dim, projection_dim)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.layer_norm = nn.LayerNorm(projection_dim)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        projected = self.projection(x)\n",
    "        x = self.gelu(projected)\n",
    "        x = self.fc(x)\n",
    "        x = self.dropout(x)\n",
    "        x = x + projected\n",
    "        x = self.layer_norm(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AvgMeter:\n",
    "    def __init__(self, name=\"Metric\"):\n",
    "        self.name = name\n",
    "        self.reset()\n",
    "\n",
    "    def reset(self):\n",
    "        self.avg_loss,self.sum_loss, self.count = [0] * 3\n",
    "\n",
    "    def update(self, loss, count=1):\n",
    "        self.count += count\n",
    "        self.sum_loss += loss * count\n",
    "        self.avg_loss = self.sum_loss / self.count\n",
    "        \n",
    "\n",
    "    def __repr__(self):\n",
    "        text = f\"{self.name}: avg_loss = {self.avg_loss:.4f}\"\n",
    "        return text\n",
    "\n",
    "def get_lr(optimizer):\n",
    "    for param_group in optimizer.param_groups:\n",
    "        return param_group[\"lr\"]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from copy import deepcopy\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Same as CLIP Projection, but implementing MOCO to be able to finetune both Text and Image tower as well, and keep a lot\n",
    "# of negative contrastive exemples despite the smaller batch size\n",
    "\n",
    "class CLIPProjMoco(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        temperature=CFG.temperature,\n",
    "        image_embedding=CFG.image_embedding,\n",
    "        text_embedding=CFG.text_embedding,\n",
    "        proj_dim = CFG.projection_dim,\n",
    "        trainable=CFG.trainable,\n",
    "        K=CFG.K,\n",
    "        m=0.999\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.image_encoder = ImageEncoder()\n",
    "        self.text_encoder = TextEncoder()\n",
    "        self.image_projection = ProjectionHead(embedding_dim=image_embedding)\n",
    "        self.text_projection = ProjectionHead(embedding_dim=text_embedding)\n",
    "        self.proj_dim = proj_dim\n",
    "        self.temperature = temperature\n",
    "        self.trainable = trainable\n",
    "\n",
    "        # MOCO parameters\n",
    "        self.K = K\n",
    "        self.m = m\n",
    "\n",
    "        # Init key encoders\n",
    "        self.image_key_encoder = deepcopy(self.image_encoder)\n",
    "        for param_k in self.image_key_encoder.parameters():param_k.requires_grad = False\n",
    "\n",
    "        self.text_key_encoder = deepcopy(self.text_encoder)\n",
    "        for param_k in self.image_key_encoder.parameters(): param_k.requires_grad = False\n",
    "\n",
    "        self.image_key_projection = deepcopy(self.image_projection)\n",
    "        for param_k in self.image_key_projection.parameters(): param_k.requires_grad = False\n",
    "\n",
    "        self.text_key_projection = deepcopy(self.text_projection)\n",
    "        for param_k in self.text_key_projection.parameters():param_k.requires_grad = False\n",
    "\n",
    "        # Init Queues\n",
    "        self.image_queue = torch.randn(self.K,self.proj_dim)\n",
    "        self.text_queue = torch.randn(self.K,self.proj_dim)\n",
    "\n",
    "        self.queue_ptr = 0\n",
    "\n",
    "    def encode_text(self,text):\n",
    "        if not self.trainable:\n",
    "            with torch.no_grad():\n",
    "                text_features = self.text_encoder(input_ids=text[\"input_ids\"], attention_mask=text[\"attention_mask\"])\n",
    "        \n",
    "        else:\n",
    "            text_features = self.text_encoder(input_ids=text[\"input_ids\"], attention_mask=text[\"attention_mask\"])\n",
    "\n",
    "        # Getting Text Embeddings (output of proj heads)\n",
    "        text_embeddings = self.text_projection(text_features)\n",
    "\n",
    "        return  text_embeddings\n",
    "    \n",
    "    def key_encode_text(self,text):\n",
    "        if not self.trainable:\n",
    "            with torch.no_grad():\n",
    "                text_features = self.text_key_encoder(input_ids=text[\"input_ids\"], attention_mask=text[\"attention_mask\"])\n",
    "        \n",
    "        else:\n",
    "            text_features = self.text_key_encoder(input_ids=text[\"input_ids\"], attention_mask=text[\"attention_mask\"])\n",
    "\n",
    "        # Getting Text Embeddings (output of proj heads)\n",
    "        text_embeddings = self.text_key_projection(text_features)\n",
    "\n",
    "        return  text_embeddings\n",
    "\n",
    "    def encode_image(self,image):\n",
    "        if not self.trainable:\n",
    "            with torch.no_grad():\n",
    "                image_features = self.image_encoder(image)\n",
    "\n",
    "        \n",
    "        else:\n",
    "            image_features = self.image_encoder(image)\n",
    "\n",
    "\n",
    "        # Getting Image Embeddings (output of proj heads)\n",
    "        image_embeddings = self.image_projection(image_features)\n",
    "\n",
    "\n",
    "        return image_embeddings\n",
    "\n",
    "    def key_encode_image(self,image):\n",
    "        if not self.trainable:\n",
    "            with torch.no_grad():\n",
    "                image_features = self.image_key_encoder(image)\n",
    "\n",
    "        \n",
    "        else:\n",
    "            image_features = self.image_key_encoder(image)\n",
    "\n",
    "\n",
    "        # Getting Image Embeddings (output of proj heads)\n",
    "        image_embeddings = self.image_key_projection(image_features)\n",
    "\n",
    "\n",
    "        return image_embeddings\n",
    "\n",
    "    ## Update all key parameters (both encoders and projection module)\n",
    "    def _momentum_update_key_encoders(self):\n",
    "        for param_q, param_k in zip(self.image_encoder.parameters(), self.image_key_encoder.parameters()):\n",
    "            param_k.data = param_k.data * self.m + param_q.data * (1. - self.m)\n",
    "            \n",
    "        for param_q, param_k in zip(self.text_encoder.parameters(), self.text_key_encoder.parameters()):\n",
    "            param_k.data = param_k.data * self.m + param_q.data * (1. - self.m)\n",
    "    \n",
    "        for param_q, param_k in zip(self.image_projection.parameters(), self.image_key_projection.parameters()):\n",
    "            param_k.data = param_k.data * self.m + param_q.data * (1. - self.m)\n",
    "            \n",
    "        for param_q, param_k in zip(self.text_projection.parameters(), self.text_key_projection.parameters()):\n",
    "            param_k.data = param_k.data * self.m + param_q.data * (1. - self.m)\n",
    "    \n",
    "    # Add new minibatch _k to queue and remove the oldest minibatch in queue\n",
    "    def _dequeue_and_enqueue(self, image_k, text_k):\n",
    "        bs = image_k.size(0)\n",
    "        assert self.K % bs == 0  # for simplicity\n",
    "        self.image_queue[self.queue_ptr:self.queue_ptr+bs, :] = image_k\n",
    "        self.text_queue[self.queue_ptr:self.queue_ptr+bs, :] = text_k\n",
    "        self.queue_ptr = (self.queue_ptr + bs) % self.K  # move pointer\n",
    "\n",
    "\n",
    "    def forward(self, image,text):\n",
    "      \n",
    "        image_embeddings = self.encode_image(image)\n",
    "        text_embeddings = self.encode_text(text)\n",
    "\n",
    "        return {\"image_embed\": image_embeddings, \"text_embed\": text_embeddings}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_one_MOCO_epoch(model, loss_fn, train_loader, optimizer,device):\n",
    "    \n",
    "    loss_meter = AvgMeter()\n",
    "\n",
    "    tqdm_object = tqdm(train_loader, total=len(train_loader))\n",
    "    \n",
    "    \n",
    "\n",
    "    \n",
    "\n",
    "    \n",
    "    for batch in tqdm_object:\n",
    "\n",
    "        image = batch[\"image\"].to(device)\n",
    "        text = {\"input_ids\": batch[\"input_ids\"].to(device), \"attention_mask\": batch[\"attention_mask\"].to(device)}\n",
    "        \n",
    "        # Update the momentum encoder\n",
    "        # Generate key for this batch, and update the queue\n",
    "        with torch.no_grad():\n",
    "\n",
    "            model._momentum_update_key_encoders()\n",
    "\n",
    "            \n",
    "            key_image_features = model.key_encode_image(image)\n",
    "            key_text_features = model.key_encode_text(text)\n",
    "\n",
    "            key_image_features = key_image_features / key_image_features.norm(dim=-1, keepdim=True)\n",
    "            key_text_features = key_text_features / key_text_features.norm(dim=-1, keepdim=True)\n",
    "\n",
    "            model._dequeue_and_enqueue(key_image_features,key_text_features)\n",
    "        \n",
    "        # Now the keys are the updated queue\n",
    "        keys_for_this_batch = {\"image_embed\" : model.image_queue.to(device), \"text_embed\": model.text_queue.to(device)}\n",
    "        \n",
    "        \n",
    "        # Zero your gradients for every batch!\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        #compute prediction for the batch\n",
    "        output = model(image,text)\n",
    "        \n",
    "        \n",
    "        #compute loss and its gradients\n",
    "        loss = loss_fn(output,keys_for_this_batch)\n",
    "        loss.backward()\n",
    "\n",
    "        # Adjust learning weights\n",
    "        optimizer.step()\n",
    "\n",
    "        \n",
    "\n",
    "        # Gather data and report\n",
    "        count = batch[\"image\"].size(0)\n",
    "        loss_meter.update(loss, count)\n",
    "\n",
    "        wandb.log({\"loss\": loss_meter.avg_loss, \"lr\" : get_lr(optimizer)  } )\n",
    "        tqdm_object.set_postfix(train_loss=loss_meter.avg_loss.item())\n",
    "        \n",
    "        \n",
    "    return loss_meter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dataloader import get_dataloader\n",
    "from tokenizer import get_tokenizer,get_feature_extractor\n",
    "from losses import CLIPMoCOLoss, CLIPLoss\n",
    "import itertools\n",
    "from training import valid_one_epoch\n",
    "from transformers import logging\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mcarrelv\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "wandb version 0.13.4 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.13.1"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/home/vincent/unifr/master_thesis/master_lit/wandb/run-20221024_180212-2p7g6u81</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href=\"https://wandb.ai/carrelv/master_test_1/runs/2p7g6u81\" target=\"_blank\">Moco</a></strong> to <a href=\"https://wandb.ai/carrelv/master_test_1\" target=\"_blank\">Weights & Biases</a> (<a href=\"https://wandb.me/run\" target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using downloaded and verified file: ./flickr30k/flickr30k_train.json\n",
      "Using downloaded and verified file: ./flickr30k/flickr30k_val.json\n",
      "Epoch: 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 453/453 [06:41<00:00,  1.13it/s, train_loss=6.89]\n",
      "100%|██████████| 15/15 [00:06<00:00,  2.32it/s, valid_loss=4.16]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 453/453 [06:41<00:00,  1.13it/s, train_loss=6.92]\n",
      "100%|██████████| 15/15 [00:06<00:00,  2.16it/s, valid_loss=4.16]\n"
     ]
    }
   ],
   "source": [
    "logging.set_verbosity_error()\n",
    "\n",
    "wandb.init(project=\"master_test_1\",\n",
    "           config={\n",
    "               \"batch_size\": CFG.batch_size,\n",
    "               \"learning_rate\": CFG.head_lr,\n",
    "               \"dataset\": \"flickr30k\",\n",
    "           },\n",
    "           group=\"group_test\",\n",
    "           name=\"Moco\")\n",
    "tokenizer = get_tokenizer(CFG.text_model_name)\n",
    "feature_extractor = get_feature_extractor(CFG.image_model_name)\n",
    "\n",
    "dataloader_train = get_dataloader(tokenizer=tokenizer,feature_extractor=feature_extractor,batch_size=CFG.batch_size,shuffle=CFG.shuffle_train,num_workers=CFG.num_workers,split=\"train\")\n",
    "dataloader_valid = get_dataloader(tokenizer=tokenizer,feature_extractor=feature_extractor,batch_size=CFG.batch_size,shuffle=CFG.shuffle_train,num_workers=CFG.num_workers,split=\"val\")\n",
    "\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "\n",
    "model = CLIPProjMoco().to(device)\n",
    "loss_train = CLIPMoCOLoss()\n",
    "loss_valid = CLIPLoss()\n",
    "if CFG.trainable == False:\n",
    "        params = [\n",
    "            {\"params\": itertools.chain(\n",
    "                model.image_projection.parameters(), model.text_projection.parameters()\n",
    "            ), \"lr\": CFG.head_lr, \"weight_decay\": CFG.weight_decay}\n",
    "        ]\n",
    "else: \n",
    "    params = [\n",
    "        {\"params\": model.image_encoder.parameters(), \"lr\": CFG.image_encoder_lr},\n",
    "        {\"params\": model.text_encoder.parameters(), \"lr\": CFG.text_encoder_lr},\n",
    "        {\"params\": itertools.chain(\n",
    "            model.image_projection.parameters(), model.text_projection.parameters()\n",
    "        ), \"lr\": CFG.head_lr, \"weight_decay\": CFG.weight_decay}\n",
    "    ]\n",
    "optimizer = torch.optim.AdamW(params, weight_decay=0.)\n",
    "lr_scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer,T_max=CFG.T_max)\n",
    "\n",
    "best_loss = float('inf')\n",
    "\n",
    "for epoch in range(CFG.epochs):\n",
    "    \n",
    "    print(f\"Epoch: {epoch + 1}\")\n",
    "    model.train()\n",
    "    train_loss = train_one_MOCO_epoch(model, loss_train, dataloader_train, optimizer,device)\n",
    "    \n",
    "    model.eval()\n",
    "\n",
    "    with torch.no_grad():\n",
    "        valid_loss = valid_one_epoch(model,loss_valid,dataloader_valid,device)\n",
    "\n",
    "    if valid_loss.avg_loss < best_loss:\n",
    "        best_loss = valid_loss.avg_loss\n",
    "        torch.save(model.image_projection.state_dict(), \"weights/img_proj_best.pt\")\n",
    "        torch.save(model.text_projection.state_dict(), \"weights/text_proj_best.pt\")\n",
    "        #print(\"Saved Best Model!\")\n",
    "    \n",
    "\n",
    "    \n",
    "    lr_scheduler.step()\n",
    "\n",
    "\n",
    "torch.save(model.image_projection.state_dict(), \"weights/img_proj_last.pt\")\n",
    "torch.save(model.text_projection.state_dict(), \"weights/text_proj_last.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "f4a9f4c4ce030a60d5e9e4103a5ecbe6be356f825e2c4ec21bb33804c21bc9e1"
  },
  "kernelspec": {
   "display_name": "Python 3.10.4 ('lit')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
