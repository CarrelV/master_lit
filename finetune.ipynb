{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Baseline\n",
    "\n",
    "- pretrained Electra Small discrim as text tower\n",
    "- pretrained ViT Tiny trained with DINO on ImageNet100 as image tower\n",
    "- Compute classic CLIP loss \n",
    "- finetune on Flickr30k\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_9212/2101026369.py:9: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from tqdm.autonotebook import tqdm\n"
     ]
    }
   ],
   "source": [
    "import os, sys, random\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from PIL import Image\n",
    "from pathlib import Path\n",
    "\n",
    "from functools import partial\n",
    "import itertools\n",
    "from tqdm.autonotebook import tqdm\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from transformers import ElectraTokenizerFast, ElectraConfig, ElectraForMaskedLM, ElectraForPreTraining\n",
    "from torchvision.datasets import Flickr30k\n",
    "from torchvision import transforms \n",
    "from hugdatafast import datasets\n",
    "from hugdatafast.fastai import HF_Datasets\n",
    "from fastai.text.all import *\n",
    "from timm.models.layers import PatchEmbed\n",
    "from timm.models.vision_transformer import VisionTransformer, _cfg\n",
    "\n",
    "from utils import ELECTRADataProcessor\n",
    "\n",
    "import wandb\n",
    "from fastai.callback.wandb import WandbCallback\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.cuda.is_available()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CFG:\n",
    "    debug = False\n",
    "    image_path = \"./flickr30k-images\"\n",
    "    captions_path = \"./flickr30k/results_20130124.token\"\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    \n",
    "    base_name = \"vanilla\"\n",
    "    seed = 2022\n",
    "\n",
    "    pretrained_checkpoint_electra = None\n",
    "    # None to use model from HuggingFace\n",
    "\n",
    "    \n",
    "    adam_bias_correction_elec = False\n",
    "    size_elec = \"small\"\n",
    "\n",
    "    logger = \"wandb\"\n",
    "\n",
    "    datas = [\"flickr30k\"]\n",
    "    num_workers = 3\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "i = ['small', 'base', 'large'].index(CFG.size_elec)\n",
    "CFG.lr_elec = [3e-4, 1e-4, 5e-5][i]\n",
    "CFG.layer_lr_decay_elec = [0.8, 0.8, 0.9][i]\n",
    "CFG.max_length_elec = [128, 512, 512][i]\n",
    "\n",
    "if CFG.pretrained_checkpoint_electra is None: CFG.max_length_elec = 512 \n",
    "# All public models is ++, which use max_length 512\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# huggingface/transformers\n",
    "hf_tokenizer_elec = ElectraTokenizerFast.from_pretrained(f\"google/electra-{CFG.size_elec}-discriminator\")\n",
    "electra_config = ElectraConfig.from_pretrained(f'google/electra-{CFG.size_elec}-discriminator')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LightWandbCallback(Callback):\n",
    "    def __init__(self, run):\n",
    "        self.run = run\n",
    "    def after_epoch(self):\n",
    "        if self.epoch != (self.n_epoch - 1): return\n",
    "        wandb.log({n:s for n,s in zip(self.recorder.metric_names, self.recorder.log) if n not in ['train_loss', 'epoch', 'time']})\n",
    "    def after_fit(self):\n",
    "        wandb.log({}) # ensure sync of last step\n",
    "        self.run.finish()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "f4a9f4c4ce030a60d5e9e4103a5ecbe6be356f825e2c4ec21bb33804c21bc9e1"
  },
  "kernelspec": {
   "display_name": "Python 3.10.4 ('lit')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
