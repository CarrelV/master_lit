{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/vincent/miniconda3/envs/lit/lib/python3.10/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "\n",
    "from torchvision.datasets import VisionDataset, Flickr30k\n",
    "import torch\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "\n",
    "\n",
    "import os\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "import nltk\n",
    "from collections import Counter\n",
    "import pickle\n",
    "from tqdm import tqdm\n",
    "import re\n",
    "import torchvision.transforms as transforms\n",
    "import json\n",
    "from torchvision.datasets.utils import download_url\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "transform = transforms.Compose([\n",
    "            transforms.RandomCrop(224),\n",
    "            transforms.RandomHorizontalFlip(), \n",
    "            transforms.ToTensor(), \n",
    "            transforms.Normalize((0.444, 0.421, 0.385), \n",
    "                                 (0.285, 0.277, 0.286))])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pre_caption(caption,max_words=128):\n",
    "    caption = re.sub(\n",
    "        r\"([.!\\\"()*#:;~])\",       \n",
    "        ' ',\n",
    "        caption.lower(),\n",
    "    )\n",
    "    caption = re.sub(\n",
    "        r\"\\s{2,}\",\n",
    "        ' ',\n",
    "        caption,\n",
    "    )\n",
    "    caption = caption.rstrip('\\n') \n",
    "    caption = caption.strip(' ')\n",
    "\n",
    "    #truncate caption\n",
    "    caption_words = caption.split(' ')\n",
    "    if len(caption_words)>max_words:\n",
    "        caption = ' '.join(caption_words[:max_words])\n",
    "            \n",
    "    return caption\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class flickr30k(Dataset):\n",
    "    def __init__(self, transform, image_root, ann_root, split, max_words=128, prompt=''):        \n",
    "        '''\n",
    "        image_root (string): Root directory of images (e.g. data/)\n",
    "        ann_root (string): directory to store the annotation file\n",
    "        split (string): one of \"train\" or \"test\"\n",
    "        '''        \n",
    "        train = 'https://storage.googleapis.com/sfr-vision-language-research/datasets/flickr30k_train.json'\n",
    "        test = 'https://storage.googleapis.com/sfr-vision-language-research/datasets/flickr30k_test.json'\n",
    "        filename = 'flickr30k_train.json'\n",
    "\n",
    "        self.split = split\n",
    "        assert self.split in (\"train\",\"test\")\n",
    "\n",
    "        if self.split == \"train\":\n",
    "            url = train\n",
    "        else:\n",
    "            url = test\n",
    "\n",
    "        download_url(url,ann_root)\n",
    "        \n",
    "        self.annotation = json.load(open(os.path.join(ann_root,filename),'r'))\n",
    "        self.transform = transform\n",
    "        self.image_root = image_root\n",
    "        self.max_words = max_words\n",
    "        self.prompt = prompt\n",
    "        \n",
    "        self.img_ids = {}  \n",
    "        n = 0\n",
    "        for ann in self.annotation:\n",
    "            img_id = ann['image_id']\n",
    "            if img_id not in self.img_ids.keys():\n",
    "                self.img_ids[img_id] = n\n",
    "                n += 1    \n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.annotation)\n",
    "    \n",
    "    def __getitem__(self, index):    \n",
    "        \n",
    "        ann = self.annotation[index]\n",
    "        \n",
    "        image_path = os.path.join(self.image_root,ann['image'])        \n",
    "        image = Image.open(image_path).convert('RGB')   \n",
    "        image = self.transform(image)\n",
    "        \n",
    "        caption = self.prompt+pre_caption(ann['caption'], self.max_words)\n",
    "        \n",
    "        #return image, caption, self.img_ids[ann['image_id']] \n",
    "        return image, caption"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using downloaded and verified file: ./flickr30k/flickr30k_train.json\n"
     ]
    }
   ],
   "source": [
    "ds = flickr30k(transform=transform,image_root=\"\",ann_root=\"./flickr30k\",split=\"train\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "image,caption = ds.__getitem__(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'str'>\n",
      "81\n",
      "two young guys with shaggy hair look at their hands while hanging out in the yard\n"
     ]
    }
   ],
   "source": [
    "print(type(caption))\n",
    "print(len(caption))\n",
    "print(caption)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'torch.Tensor'>\n",
      "torch.Size([3, 224, 224])\n",
      "tensor([[[-1.5029, -1.4341, -1.5029,  ..., -1.5579, -1.4478, -1.3102],\n",
      "         [-1.4478, -1.4065, -1.4891,  ..., -1.2277, -1.0488, -1.1451],\n",
      "         [-1.4065, -1.4753, -1.4616,  ..., -0.7736, -1.0625, -1.3515],\n",
      "         ...,\n",
      "         [-1.0763, -0.4158, -0.4158,  ..., -1.3928, -0.5397, -1.4616],\n",
      "         [-0.8699, -1.0901, -0.0581,  ..., -1.4478, -0.8837, -1.3928],\n",
      "         [-1.0488, -1.2277, -1.0075,  ..., -1.3653, -1.0213, -0.2645]],\n",
      "\n",
      "        [[-1.2933, -1.1801, -1.1518,  ..., -1.1235, -1.1942, -0.6279],\n",
      "         [-1.2933, -1.1942, -1.2226,  ..., -0.8828, -0.1041, -0.0900],\n",
      "         [-1.2792, -1.1518, -1.3358,  ...,  0.1790,  0.0233, -0.5288],\n",
      "         ...,\n",
      "         [-0.9819, -0.6704, -0.6279,  ..., -0.9252, -0.0050, -1.2650],\n",
      "         [-0.7129, -1.2509, -0.2882,  ..., -0.9252, -0.5430, -0.8545],\n",
      "         [-0.9960, -1.3358, -1.1093,  ..., -1.0951, -0.8403, -0.1749]],\n",
      "\n",
      "        [[-1.2639, -1.2776, -1.2776,  ..., -1.3462, -1.2913, -1.0719],\n",
      "         [-1.2776, -1.1679, -1.1953,  ..., -0.9622, -1.0582, -1.0582],\n",
      "         [-1.2776, -1.3462, -1.2227,  ..., -0.4275, -0.8251, -1.1679],\n",
      "         ...,\n",
      "         [-1.2227, -0.9896, -0.9896,  ..., -1.0993, -0.6606, -1.3324],\n",
      "         [-1.0171, -1.3462, -0.6880,  ..., -1.1131, -1.0719, -1.2502],\n",
      "         [-1.1679, -1.0582, -0.9759,  ..., -1.0856, -1.0582, -0.4686]]])\n"
     ]
    }
   ],
   "source": [
    "print(type(image))\n",
    "print(image.shape)\n",
    "print(image)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "def collate_test(batch):\n",
    "    imgs = torch.stack([item[0] for item in batch])\n",
    "    caps = [item[1] for item in batch]\n",
    "    #for img, cap in batch:\n",
    "    \n",
    "    #    x.append(x_)\n",
    "    #    caps.append(cap)\n",
    "    return imgs, caps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 2\n",
    "shuffle_img = False\n",
    "num_workers = 2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "def collate_fn(batch):\n",
    "    imgs = torch.stack([item[0] for item in batch])\n",
    "    caps = [item[0] for item in batch]\n",
    "    imgs = [item[1] for item in batch]\n",
    "    #for img, cap in batch:\n",
    "    \n",
    "    #    x.append(x_)\n",
    "\n",
    "    #    caps.append(cap)\n",
    "    return caps, imgs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataloader = DataLoader(ds,batch_size=batch_size,shuffle=shuffle_img,collate_fn=collate_test)\n",
    "#dataloader = DataLoader(ds,batch_size=batch_size,shuffle=shuffle_img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "img, cap = next(iter(dataloader))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'list'>\n",
      "2\n",
      "['two young guys with shaggy hair look at their hands while hanging out in the yard', 'two young, white males are outside near many bushes']\n"
     ]
    }
   ],
   "source": [
    "print(type(cap))\n",
    "print(len(cap))\n",
    "print(cap)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'torch.Tensor'>\n",
      "torch.Size([2, 3, 224, 224])\n"
     ]
    }
   ],
   "source": [
    "print(type(img))\n",
    "print(img.shape)\n",
    "#print(img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import BertTokenizerFast"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_model_name = \"prajjwal1/bert-medium\"\n",
    "\n",
    "tokenizer = BertTokenizerFast.from_pretrained(text_model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoded = tokenizer.batch_encode_plus(cap,add_special_tokens=True,max_length=128,padding=\"max_length\",return_attention_mask=True,return_tensors=\"pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_ids = encoded['input_ids']\n",
    "attn_mask = encoded['attention_mask']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[  101,  2048,  2402,  4364,  2007, 25741,  2606,  2298,  2012,  2037,\n",
       "          2398,  2096,  5689,  2041,  1999,  1996,  4220,   102,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0],\n",
       "        [  101,  2048,  2402,  1010,  2317,  3767,  2024,  2648,  2379,  2116,\n",
       "         14568,   102,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0]])"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "f4a9f4c4ce030a60d5e9e4103a5ecbe6be356f825e2c4ec21bb33804c21bc9e1"
  },
  "kernelspec": {
   "display_name": "Python 3.10.4 ('lit')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
