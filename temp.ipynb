{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dataloader import get_dataloader\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DataSet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 2\n",
    "shuffle_img = False\n",
    "num_workers = 2\n",
    "split = \"train\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using downloaded and verified file: ./flickr30k/flickr30k_train.json\n"
     ]
    }
   ],
   "source": [
    "\n",
    "dataloader_train = get_dataloader(batch_size=batch_size,shuffle=shuffle_img,num_workers=num_workers,split=split)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\"\n",
    "img  = next(iter(dataloader_train))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[[[-1.4065, -1.4478, -1.2965,  ...,  1.8958,  0.1759, -1.2965],\n",
       "           [-1.4753, -1.4478, -1.3653,  ...,  1.9234,  1.9509,  1.6894],\n",
       "           [-1.4341, -1.4203, -1.3928,  ...,  1.7720,  1.8958,  1.9509],\n",
       "           ...,\n",
       "           [-0.2645, -1.5029, -0.5534,  ...,  0.3960, -0.9249, -0.8424],\n",
       "           [-0.0993, -1.2277, -0.8561,  ..., -0.8837, -0.5121, -0.1681],\n",
       "           [-0.2369, -1.5579, -1.5579,  ...,  0.0520,  0.3547,  1.4968]],\n",
       " \n",
       "          [[-1.1093, -1.0385, -0.8261,  ...,  2.0619,  0.3772, -1.0244],\n",
       "           [-1.3500, -0.9960, -0.7978,  ...,  2.0336,  2.0478,  1.7929],\n",
       "           [-1.3500, -1.0244, -0.7695,  ...,  2.0478,  2.0619,  2.0761],\n",
       "           ...,\n",
       "           [ 0.3489, -0.9111,  0.1790,  ...,  0.1224, -0.9111, -0.7695],\n",
       "           [ 0.4763, -0.4722, -0.3873,  ..., -0.8686, -0.5288,  0.1082],\n",
       "           [ 0.2356, -1.2933, -1.2650,  ...,  0.1507,  0.2073,  1.5240]],\n",
       " \n",
       "          [[-1.2502, -1.2776, -1.0582,  ...,  2.1503,  0.3130, -1.0034],\n",
       "           [-1.3462, -1.2227, -1.1131,  ...,  2.1092,  2.0955,  1.9035],\n",
       "           [-1.2776, -1.1679, -1.1679,  ...,  2.1229,  2.0818,  2.1503],\n",
       "           ...,\n",
       "           [ 0.1759, -1.3462, -1.1542,  ..., -0.2629, -1.0171, -0.7017],\n",
       "           [ 0.1347, -1.1679, -1.1268,  ..., -0.8114, -0.6057,  0.0524],\n",
       "           [-0.8114, -1.3462, -1.1542,  ...,  0.1621,  0.0524,  1.3414]]],\n",
       " \n",
       " \n",
       "         [[[-1.4753, -1.4065, -1.4478,  ..., -1.5166, -1.3928, -1.4616],\n",
       "           [-1.5441, -1.4616, -1.4478,  ..., -1.4478, -1.1451, -1.0075],\n",
       "           [-1.4753, -1.5166, -1.3928,  ..., -1.0763, -1.2001, -1.0213],\n",
       "           ...,\n",
       "           [-0.2782, -1.2414, -1.1864,  ..., -1.1313, -1.2414, -1.3515],\n",
       "           [ 0.4786, -1.1313, -0.8561,  ..., -1.1589, -1.3102, -1.3790],\n",
       "           [ 0.7675, -0.9387, -1.5579,  ..., -0.5947, -1.1038, -1.0213]],\n",
       " \n",
       "          [[-1.2367, -1.2084, -1.4632,  ..., -1.4066, -1.2084, -1.2792],\n",
       "           [-1.2367, -1.3358, -1.4208,  ..., -1.0951, -0.4581, -0.4014],\n",
       "           [-1.1093, -1.4632, -1.4066,  ..., -0.3590, -0.3023, -0.1041],\n",
       "           ...,\n",
       "           [-0.2740, -0.5713, -0.4156,  ..., -0.7695, -0.9394, -1.1518],\n",
       "           [ 0.8161, -0.9819, -0.4156,  ..., -0.8969, -1.0527, -1.2367],\n",
       "           [ 1.3258, -0.8969, -1.4208,  ..., -0.4014, -0.9394, -0.8261]],\n",
       " \n",
       "          [[-1.2776, -1.2365, -1.3324,  ..., -1.3324, -1.1953, -1.2639],\n",
       "           [-1.3462, -1.3050, -1.3187,  ..., -1.2227, -1.1268, -1.0445],\n",
       "           [-1.2227, -1.3462, -1.2776,  ..., -0.8251, -1.0171, -0.9211],\n",
       "           ...,\n",
       "           [-0.4138, -1.1542, -1.0582,  ..., -1.1131, -1.1131, -1.1816],\n",
       "           [-0.0572, -0.9348, -0.8937,  ..., -1.1405, -1.2090, -1.3187],\n",
       "           [-0.0847, -0.6743, -1.3462,  ..., -1.0171, -1.2913, -1.0171]]]]),\n",
       " ['two young guys with shaggy hair look at their hands while hanging out in the yard',\n",
       "  'two young, white males are outside near many bushes'])"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import BertTokenizerFast,ViTFeatureExtractor, BertModel, ViTModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "vision_model_name = \"facebook/dino-vits16\"\n",
    "text_model_name = \"prajjwal1/bert-medium\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = BertTokenizerFast.from_pretrained(text_model_name)\n",
    "feature_extractor = ViTFeatureExtractor.from_pretrained(vision_model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of ViTModel were not initialized from the model checkpoint at facebook/dino-vits16 and are newly initialized: ['pooler.dense.weight', 'pooler.dense.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "vision_model = ViTModel.from_pretrained(vision_model_name)\n",
    "#vision_model.config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading: 100%|██████████| 159M/159M [00:02<00:00, 82.8MB/s] \n",
      "Some weights of the model checkpoint at prajjwal1/bert-medium were not used when initializing BertModel: ['cls.predictions.transform.dense.bias', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.decoder.bias']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "BertConfig {\n",
       "  \"_name_or_path\": \"prajjwal1/bert-medium\",\n",
       "  \"attention_probs_dropout_prob\": 0.1,\n",
       "  \"classifier_dropout\": null,\n",
       "  \"hidden_act\": \"gelu\",\n",
       "  \"hidden_dropout_prob\": 0.1,\n",
       "  \"hidden_size\": 512,\n",
       "  \"initializer_range\": 0.02,\n",
       "  \"intermediate_size\": 2048,\n",
       "  \"layer_norm_eps\": 1e-12,\n",
       "  \"max_position_embeddings\": 512,\n",
       "  \"model_type\": \"bert\",\n",
       "  \"num_attention_heads\": 8,\n",
       "  \"num_hidden_layers\": 8,\n",
       "  \"pad_token_id\": 0,\n",
       "  \"position_embedding_type\": \"absolute\",\n",
       "  \"transformers_version\": \"4.18.0\",\n",
       "  \"type_vocab_size\": 2,\n",
       "  \"use_cache\": true,\n",
       "  \"vocab_size\": 30522\n",
       "}"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text_model = BertModel.from_pretrained(text_model_name)\n",
    "text_model.config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs = tokenizer(\"Hello, my dog is cute\", return_tensors=\"pt\")\n",
    "outputs = text_model(**inputs)\n",
    "\n",
    "last_hidden_states = outputs.last_hidden_state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': tensor([[  101,  7592,  1010,  2026,  3899,  2003, 10140,   102]]), 'token_type_ids': tensor([[0, 0, 0, 0, 0, 0, 0, 0]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1]])}"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[ 0.5313, -0.0959,  1.1669,  ..., -0.4262,  0.4165, -0.2875],\n",
       "         [ 0.0024, -0.7301,  0.5934,  ..., -0.4460,  1.2140,  0.8000],\n",
       "         [-0.2985,  0.1856,  0.4116,  ..., -0.0307,  0.8842, -0.3856],\n",
       "         ...,\n",
       "         [ 0.0987,  0.4613,  0.8336,  ..., -0.5302,  0.4894,  0.3599],\n",
       "         [ 0.2055,  0.2889,  1.0970,  ..., -0.1850,  0.6020, -0.0864],\n",
       "         [ 0.3357, -0.0997,  1.6814,  ..., -0.6950,  1.0993,  0.0910]]],\n",
       "       grad_fn=<NativeLayerNormBackward0>)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "last_hidden_states"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "f4a9f4c4ce030a60d5e9e4103a5ecbe6be356f825e2c4ec21bb33804c21bc9e1"
  },
  "kernelspec": {
   "display_name": "Python 3.10.4 ('lit')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
